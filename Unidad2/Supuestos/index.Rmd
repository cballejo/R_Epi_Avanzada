---
title: "**Análisis de normalidad y homocedasticidad con R**"
author: ""
date: ""
output:
  html_document:
    css: style.css
    toc: true
    toc_float: true
    toc_collapsed: false
    toc_depth: 4
number_sections: true
anchor_sections: true
theme: lumen
---

```{r, message=FALSE, echo=F}
knitr::opts_chunk$set(comment=NA, dpi = 300)
```

<br>

<center>

*Este material es parte de la* ***Unidad 2 del Curso de Epidemiología - Nivel Avanzado del Instituto Nacional de Epidemiología "Dr. Juan H. Jara" - ANLIS***

</center>

<br>

<center><p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><a property="dct:title" rel="cc:attributionURL" href="https://cballejo.github.io/R_Epi_Avanzada/Unidad2/Supuestos/">Análisis de normalidad y homocedasticidad con R</a> by <a rel="cc:attributionURL dct:creator" property="cc:attributionName" href="http://www.ine.gov.ar">Christian Ballejo</a> is licensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-NC 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"></a></p></center>

<br> 

## Introducción

Este material es una continuación del documento [Análisis exploratorio de datos](https://cballejo.github.io/R_Epi_Avanzada/EDA/). 


Uno de los objetivos del análisis exploratorio de datos es conocer cómo se distribuyen los valores de las variables de interés. Entre las características más relevantes, a la hora de la inferencia, es saber si dicha distribución se ajusta a la "normal" y si su dispersión es homogénea.

Ahora que vimos cómo funcionan los andamiajes de los test de hipótesis podemos avanzar en el uso de ciertas **pruebas de bondad de ajuste** necesarias para evaluar estos supuestos de normalidad y homocedasticidad.

## Normalidad

Determinar que una distribución es aproximadamente normal nos permite decidirnos por test de comparaciones paramétricos. 

Existen tres enfoques que debemos analizar simultáneamente:

- Métodos gráficos 
- Métodos analíticos
- Pruebas de bondad de ajuste

### Métodos gráficos

El gráfico por excelencia para evaluar normalidad es el Cuantil-Cuantil (Q-Q Plot) que consiste en comparar los cuantiles de la distribución observada con los cuantiles teóricos de una distribución normal con la misma media y desviación estándar que los datos.

Cuanto más se aproximen los datos a una normal, más alineados están los puntos entorno a la recta.

En el lenguaje R hay varias formas de hacerlos pero aquí vamos a mostrar una bien simple con el paquete **DataExplorer**.

La función se llama `plot_qq()` y lleva como argumento obligatorio la variable cuantitativa que vamos a graficar.

```{r, echo= F, message=F, warning=F}
datos <- read.csv2("datos_normalidad.csv")
```

```{r, out.width="50%", fig.align="center",message=F, warning=F}
library(DataExplorer)

plot_qq(datos$edad, title = "edad") # title es opcional

plot_qq(datos$peso, title = "peso") # title es opcional
```

A simple vista observamos que los puntos de la variable **peso** *se ajustan a la recta* de mejor forma que la distribución de la variable **edad** (*notoriamente curvada*).

Otro paquete interesante para observar "normalidad" mediante gráficos es **ggpubr**.

```{r, out.width="50%", fig.align="center", message=F, warning=F}
library(ggpubr)

ggqqplot(datos$edad) 

ggqqplot(datos$peso) 
```

La función `ggqqplot()` agrega además un intervalo de confianza (zona gris alrededor de la recta) que nos orienta mejor sobre "donde caen" los puntos de la variable analizada.

### Métodos analíticos

**Medidas de forma**

Existen dos medidas de forma útiles que podemos calcular mediante funciones de R.

- La curtosis (kurtosis)
- La asimetría (skewness)

La curtosis mide el grado de agudeza o achatamiento de una distribución con relación a la distribución normal.

- \< 0 Distribución platicúrtica (apuntamiento negativo): baja concentración de valores
- \> 0 Distribución leptocúrtica (apuntamiento positivo): gran concentración de valores
- \= 0 Distribución mesocúrtica (apuntamiento normal): concentración como en la distribución normal.

El paquete **moments** posee algunas funciones interesantes para analizar medidas de forma.

```{r, message=F, warning=F}
library(moments)

kurtosis(datos$edad, na.rm = T)

kurtosis(datos$peso, na.rm = T)
```
En los dos casos estamos frente a una *distribución leptocúrtica* pero de magnitudes bien diferentes. Muy alta en el caso de la variable **edad** (8,0) y mucho menor para la variable **peso** (2,7).

El índice de asimetría es un indicador que permite establecer el *grado de asimetría* que presenta una distribución. Los valores menores que 0 indican distribución asimétrica negativa; los mayores a 0: distribución asimetrica positiva y cuando sea 0, o muy próximo a 0, distribución simétrica.

```{r}
skewness(datos$edad, na.rm = T)

skewness(datos$peso, na.rm = T)
```
Los valores obtenidos con la función `skewness` del paquete **moments** nos informan que la distribución de la edad tienen una asimetría positiva (2,2) y que los valores de peso se distribuyen bastante simétricos (0,1).

Estas características de las distribuciones también se pueden ver mediante gráficos de densidad.

```{r, out.width="50%", fig.align="center", message=F, warning=F}
plot_density(datos$edad) # plot_density() pertenece a DataExplorer

plot_density(datos$peso) # plot_density() pertenece a DataExplorer
```


### Pruebas de bondad de ajuste

Una prueba de bondad de ajuste permite testear la hipótesis de que una variable aleatoria sigue cierta distribución de probabilidad y se utiliza en situaciones donde se requiere comparar una distribución observada con una teórica o hipotética.

El mecanismo es idéntico a cualquier test de hipótesis salvo que aquí esperamos no descartar la hipótesis nula de igualdad, por lo que obtener valores $p$ de probabilidad mayores a 0,05 es signo de que la distribución de la variable analizada se ajusta.

A continuación, presentaremos los test de hipótesis más utilizados para analizar normalidad.

- Test de Shapiro-Wilk

Lleva el nombre de sus [autores](https://es.wikipedia.org/wiki/Test_de_Shapiro%E2%80%93Wilk) (Samuel Shapiro y Martin Wilk) y es usado preferentemente para muestras de hasta 50 observaciones.

La función se encuentra desarrollada en R base (paquete stats) y se llama `shapiro.test()`

```{r}
shapiro.test(datos$edad)

shapiro.test(datos$peso)
```
**Interpretación**: Siendo la hipótesis nula que la población está distribuida normalmente, si el p-valor es menor a $\alpha$ (nivel de significancia, convencionalmente un 0,05) entonces la hipótesis nula es rechazada (se concluye que los datos no vienen de una distribución normal). Si el p-valor es mayor a $\alpha$, se concluye que no se puede rechazar dicha hipótesis.

En función de la interpretación (que es común a todos los test de hipótesis de normalidad) podemos decir que la variable **edad** *no se ajusta a la normal* y que no podemos rechazar el ajuste de la variable **peso**.

- Test de Kolmogorov-Smirnov

El test de [Kolmogorov-Smirnov](https://es.wikipedia.org/wiki/Prueba_de_Kolmogorov-Smirnov) permite estudiar si una muestra procede de una población con una determinada distribución que no está limitado únicamente a la distribución normal.

El test asume que se conoce la media y varianza poblacional, lo que en la mayoría de los casos no es posible. Para resolver este problema, se realizó una modificación conocida como test Lilliefors. 

- Test de Lilliefors

El test de [Lilliefors](https://es.wikipedia.org/wiki/Prueba_de_Lilliefors) asume que la media y varianza son desconocidas y está especialmente desarrollado para contrastar la normalidad. 

Es la alternativa al test de Shapiro-Wilk cuando el número de observaciones es mayor de 50. 

La función `lillie.test()` del paquete **nortest** permite aplicarlo.

```{r}
library(nortest)

lillie.test(datos$edad)

lillie.test(datos$peso)
```

Los resultados son coincidentes con los obtenidos anteriormente.

- Test de D´agostino

Esta prueba se basa en las transformaciones de la curtosis y la asimetría de la muestra, y solo tiene poder frente a las alternativas de que la distribución sea sesgada.

El paquete **moments** la tiene implementada en `agostino.test()`.

```{r}
agostino.test(datos$edad)

agostino.test(datos$peso)
```

Los resultados coinciden con la observación de asimetría que efectuamos con los métodos analíticos, confirmando que la variable **edad** *no se ajusta* a una curva simétrica y la variable **peso** si lo hace.

Cuando estos test se emplean con la finalidad de verificar las condiciones de métodos paramétricos es importante tener en cuenta que, al tratarse de valores probabilidad, cuanto mayor sea el tamaño de la muestra más poder estadístico tienen y más fácil es encontrar evidencias en contra de la hipótesis nula de normalidad. 

Por otra parte, cuanto mayor sea el tamaño de la muestra, menos sensibles son los métodos paramétricos a la falta de normalidad. Por esta razón, es importante no basar las conclusiones únicamente en los resultados de los test, sino también considerar los otros métodos (gráfico y analítico) y no olvidar el tamaño de la muestra.

## Homocedasticidad

La homogeneidad de varianzas es un supuesto que considera constante la varianza en los distintos grupos que queremos comparar.

Esta homogeneidad es condición necesaria antes de aplicar algunos test de hipótesis de comparaciones o bien para aplicar correcciones mediante los argumentos de las funciones de R.

Existen diferentes test de bondad de ajuste que permiten evaluar la distribución de la varianza. Todos ellos consideran como $H_0$ que la varianza es igual entre los grupos y como $H_1$ que no lo es. 

La diferencia entre ellos es el estadístico de centralidad que utilizan:

- Media de la varianza: son los más potentes pero se plican en distribuciones que se aproximan a la normal.

- Mediana de la varianza: son menos potentes pero consiguen mejores resultados en distribuciones asimétricas.

### F-test

Este test es un contraste de la razón de varianzas, mediante el estadístico F que sigue una distribución F-Snedecor.

Se utiliza cuando las distribuciones se aproximan a la "normal" y en R base se la encuentra en la función `var.test()` 

```{r}
var.test(datos$peso[datos$sexo == "Mujer"], datos$peso[datos$sexo == "Varon"])
```
Comparamos las varianzas de la variable peso entre el grupo de mujeres y hombres. El valor $p$ del test indica que no podemos descartar la igualdad de varianzas entre los grupos ($H_0$) o lo que es lo mismo el test no encuentra diferencias significativas entre las varianzas de los dos grupos.

### Test de Bartlett

Este test se puede utilizar como alternativa al F-test, sobre todo porque nos permite aplicarlo cuando tenemos más de 2 grupos de comparación. Al igual que el anterior es sensible a las desviaciones de la normalidad.

La función en R base es `bartlett.test()` y se pueden usar argumentos tipo fórmula:

variables cuantitativa ~ variable categórica (grupos)

```{r}
bartlett.test(datos$peso ~ datos$sexo)
```
El resultado es coincidente con el mostrado por var.test(). No se encuentran diferencias significativas entres las varianzas de los pesos en los dos grupos (Mujer - Varon)


### Test de Levene

El test de Levene sirve para comparar la varianza de 2 o más grupos pero además permite elegir distintos estadísticos de tendencia central. Por lo tanto, la podemos adaptar a distribuciones alejadas de la normalidad seleccionando por ejemplo la mediana.

La función `leveneTest()` se encuentra disponible en el paquete **car**. La vemos aplicada sobre **peso** para los diferentes grupos de **sexo** y utilizando la **media** como estadístico de centralidad, dado que la distribución de peso se aproxima a la normal.

```{r, message=F, warning=FALSE}
library(car)

leveneTest(y = datos$peso, group = datos$sexo, center = "mean")
```

La conclusión es la misma que la encontrada anteriormente.

Ahora vamos aplicarla sobre la variable edad, de la que habíamos descartado "normalidad". Lo hacemos usando el argumento center con "mean" (media) y con "median" (mediana).

```{r, message=F, warning=FALSE}
leveneTest(y = datos$edad, group = datos$sexo, center = "mean")

leveneTest(y = datos$edad, group = datos$sexo, center = "median")
```

Los resultados son diferentes. Mientras con el centrado en la media nos da un $p$ valor significativo menor a 0,05 con el centrado en la mediana no nos permite descartar homocedasticidad.

Observamos aquí las distorsiones sobre la media y las formas paramétricas que devienen de distribuciones asimétricas y alejadas de la curva normal.

El código correcto para este caso (variable edad) es usar el centrado en la mediana (center = "median").



## Bibliografía 

Boxuan Cui (2020). DataExplorer: Automate Data Exploration and Treatment. R
package version 0.8.2. https://CRAN.R-project.org/package=DataExplorer

Alboukadel Kassambara (2020). ggpubr: 'ggplot2' Based Publication Ready
Plots. R package version 0.4.0. https://CRAN.R-project.org/package=ggpubr

Lukasz Komsta and Frederick Novomestky (2015). moments: Moments, cumulants,
skewness, kurtosis and related tests. R package version 0.14.
https://CRAN.R-project.org/package=moments

Juergen Gross and Uwe Ligges (2015). nortest: Tests for Normality. R package
version 1.0-4. https://CRAN.R-project.org/package=nortest

John Fox and Sanford Weisberg (2019). An {R} Companion to Applied Regression,
Third Edition. Thousand Oaks CA: Sage. URL:
https://socialsciences.mcmaster.ca/jfox/Books/Companion/