---
title: "**Regresión Lineal Múltiple**"
author: ""
date: ""
output:
  html_document:
    css: style.css
    toc: true
    toc_float: true
    toc_collapsed: false
    toc_depth: 4
number_sections: true
anchor_sections: true
theme: lumen
editor_options: 
  markdown: 
    wrap: 72
---

```{r, message=FALSE, echo=F}
knitr::opts_chunk$set(comment=NA, dpi = 300)
```

<br>

<center>

*Este material es parte de la* ***Unidad 3 del Curso de Epidemiología -
Nivel Avanzado del Instituto Nacional de Epidemiología "Dr. Juan H.
Jara" - ANLIS***

</center>

<br>

<center>

<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/">

<a property="dct:title" rel="cc:attributionURL" href="https://cballejo.github.io/R_Epi_Avanzada/Unidad3/RLM/">Regresión Lineal Múltiple</a> by
<a rel="cc:attributionURL dct:creator" property="cc:attributionName" href="http://www.ine.gov.ar">Andrea
Silva y Christian Ballejo</a> is licensed under
<a href="http://creativecommons.org/licenses/by-nc/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC
BY-NC
4.0<img src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" style="height:22px!important;margin-left:3px;vertical-align:text-bottom;"/><img src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" style="height:22px!important;margin-left:3px;vertical-align:text-bottom;"/><img src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1" style="height:22px!important;margin-left:3px;vertical-align:text-bottom;"/></a>

</p>

</center>

<br>

## Introducción

Los modelos de Regresión Lineal Múltiple (RLM) son típicamente empleados cuando la variable respuesta (o dependiente) es continua. Las variables independientes (variables explicativas o co variables) pueden ser tanto continuas como categóricas. A su vez las variables categóricas pueden ser dicotómicas, ordinales o tener múltiples niveles, siendo tratadas, en esta situación, como variables *dummy* (según veremos más adelante).

Así como la RLS nos permite estimar el efecto bruto de una variable independiente sobre una variable respuesta, la RLM nos permite conocer el efecto conjunto de 2 o más variables independientes ($X_1$, $X_2$,...$X_k$) sobre la variable respuesta ($Y$). De esta manera podemos decir que la RLM nos permite:


- Analizar  la dirección y fuerza de la asociación entre la variable dependiente y las variables independientes.

- Determinar cuáles variables independientes son importantes en la **PREDICCIÓN / EXPLICACIÓN**  de la variable dependiente

- Describir la relación entre una o más variables independientes  controlando por el efecto de las otras variables independientes $\rightarrow$ evaluamos **CONFUSIÓN**

- Identificar si la relación de la variable respuesta y una variable independiente cambia de acuerdo al nivel de otra variable independiente $\rightarrow$ evaluamos **INTERACCION**

El modelo estadístico de la RLS que expresa la relación entre $X$ e $Y$ es:

$$Y = \beta_0 + \beta_1X_1  $$

La representación gráfica de dicha relación es una recta de ajuste que se realiza en un plano (2 dimensiones).

El modelo estadístico de la RLM es:

::: {.b--gray .ba .bw2 .ma2 .pa4 .shadow-1}

$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ...+\beta_kX_k  $$

:::

Donde $\beta_0$, $\beta_1$, $\beta_2$,...,$\beta_k$ son los parámetros de la regresión. Para cada combinación de valores de $X_1$, $X_2$,...$X_k$ existe una distribución $Y$ cuya **media** es una función lineal de $X_1$, $X_2$,..., $X_k$.


```{r,echo=F,  fig.align='center', out.width = "60%"}
knitr::include_graphics("captura1.PNG")
```

La representación gráfica de la recta de ajuste se realiza en el espacio de dimensión $K + 1$ ($K$ es el número de variables). Recordamos que en el caso de la RLS, podíamos representarla en un plano (2 dimensiones), en el caso de la RLM, se nos dificulta la representación espacial si el modelo tuviera más de 2 variables.

En el caso puntual que el modelo tuviera 2 variables independientes, la ecuación sería: $Y = \beta_0 + \beta_1X_1 + \beta_2X_2$

Y podríamos representarlo:

```{r,echo=F,  fig.align='center', out.width = "60%"}
knitr::include_graphics("captura2.PNG")
```

En forma similar a la RLS, la interpretación de cada parámetro$\beta$ de la regresión es:

- $\beta_0$: es el valor esperado de $Y$ cuando todas las otras variables son iguales a cero.

-	$\beta_1$ es la pendiente a lo largo del eje $X_1$ y representa el cambio esperado en la respuesta por unidad de cambio en $X_1$ a valores constantes de $X_2$.

-	$\beta_2$ es la pendiente a lo largo del eje $X_2$ y representa el cambio esperado en la respuesta por unidad de cambio en $X_2$ a valores constantes de $X_1$.

### Presupuestos del modelo de RLM

**1. Independencia**: las observaciones $Y_i$ son independientes unas de otras: el efecto de $X_1$ sobre la respuesta media no depende de $X_2$ y viceversa, siempre y cuando no exista interacción. Cuando existe interacción entre $X_1$ e $X_2$ , el efecto de $X_1$ sobre la respuesta media de $Y$ depende $X_2$ y viceversa ($X_1$ e $X_2$  no son independientes cuando existe interacción).

**2. Linealidad**: para cada combinación de valores de las variables independientes ($X_1$, $X_2$,..., $X_k$) el valor medio de $Y$ es función lineal de $X_1$, $X_2$,...,$X_k$.. La linealidad se define en relación a los coeficientes de la regresión, por lo tanto el modelo puede incluir términos cuadráticos e interacciones

$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_1X_2\rightarrow$ modelo con interacción

$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_1^2 + \beta_4X_2^2\rightarrow$ modelo con términos cuadráticos

**3. Homocedasticidad**: la varianza de $Y$ para los distintos valores de $X_1$, $X_2$,...,$X_k$ se mantiene constante

**4. Normalidad**: Los valores de $Y$ tienen una distribución normal según los valores de  $X_1$, $X_2$, $X_k\rightarrow$ ésto nos permite realizar inferencias en relación a los parámetros del modelo.


Al igual que en la RLS la estimación de los parámetros de la regresión (coeficientes) se realiza mediante el **Método de los Mínimos Cuadrados** (MMC).

El MMC consiste en adoptar como estimativas de los parámetros de la regresión los valores que minimicen la suma de los cuadrados de los residuos

$$\sum_{i=1}^{i=n}e_1^2=\sum_{i=1}^{i=n}(Y_i-\hat{Y_i})^2=\sum_{i=n}^{i=n}(Y_i-(\hat{\beta}_0+\hat{\beta}_1X_1+\dots+\hat{\beta}_kX_k))^2 $$

Comenzaremos aprendiendo a interpretar un modelo de RLM, para luego aprender a construirlos. Para ello, observemos detalladamente el resultado de un modelo de RLM en R, donde modelamos la V23, en función de las variables V10, V11, V12, V14, V15, V17, V18 y V24.

```{r,echo=F,  fig.align='center', out.width = "100%"}
knitr::include_graphics("captura3.PNG")
```


La primera columna muestra los coeficientes estimados por el modelo de RLM, la segunda el error estándar de cada coeficiente, las siguientes columnas muestran el estadístico $F$ parcial y su $p$ valor. 

En el párrafo final  encontramos: el error estándar de los residuos, el $R_2$ múltiple y el ajustado, y el estadístico de un test $F$ global con su $p$ valor. Profundicemos ahora en lo que significa cada uno de estos puntos.

**Test F parcial**

Como estamos sacando conclusiones partiendo de una muestra, es obvio que distintas muestras van a dar distintos valores de los parámetros. Es por eso que el test $F$ parcial, testea la siguiente afirmación o hipótesis:

$H_0=\beta_1=\beta_2=\dots\beta_n=0 $

$H_1=\exists\beta_i\neq0$ (existe al menos un coeficiente $\neq$ 0)

De alguna forma, evalúa la contribución de cada variable al modelo. Nos dice si la inclusión de esa variable es útil para explicar significativamente la variabilidad observada en $Y$.  En los modelos lineales generalizados (MLG), el *test de Wald* testea esta $H_0$. Para RLM, el test $F$ parcial es idéntico a Wald.

**Varianza residual**

Como vimos en ANOVA y al igual que en el caso de regresión lineal simple, vamos a descomponer la variabilidad de la variable dependiente Y en dos componentes o fuentes de variabilidad: un componente va a representar la variabilidad explicada por el modelo de regresión y el otro componente va a representar la variabilidad no explicada por el modelo y, por tanto, atribuida a factores aleatorios.

Variabilidad total = Variabilidad Regresión + Variabilidad residual 

Suma de cuadrados totales (SCT) = Suma de cuadrados de la regresión (SCR) + Suma de cuadrados Residuales (SCE)

$$\sum (y_i-\bar{y})^2 = \sum (\hat{y}-\bar{y})^2 + \sum (y_i -\hat{y}_i)^2 $$

$$\frac{Suma \; de \; cuadrados}{totales \; (SCT)} \; \frac{Suma \; de \; cuadrados}{de \; la \; regresion \; (SCR)} \; \frac{Suma \; de \; cuadrados}{residuales \; (SCE)}$$
Recordemos que cada uno de estos términos, se divide por sus grados de libertad para obtener los cuadrados medios correspondientes (CMT/CMR/CME)


<center><style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0lax{text-align:left;vertical-align:top}
.tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-0lax">   </th>
    <th class="tg-amwm">  Grados de libertad   </th>
    <th class="tg-amwm">   CM </th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-amwm">  SCT   </td>
    <td class="tg-0lax">n-1   <br></td>
    <td class="tg-0lax">CMT=SCT/n-1   </td>
  </tr>
  <tr>
    <td class="tg-amwm">SCR   </td>
    <td class="tg-0lax">k-1   </td>
    <td class="tg-0lax">CMR=SCR/k-1   </td>
  </tr>
  <tr>
    <td class="tg-amwm">SCE   </td>
    <td class="tg-0lax">n-k-1   </td>
    <td class="tg-0lax">CME= SCE/n-k-1   </td>
  </tr>
</tbody>
</table></center>

**Test F global**

Compara el modelo de regresión con el modelo nulo. Evalúa el efecto conjunto de las variables independientes incluidas en el modelo ajustado.

$F = CMR/CME \; \; \; \;gl= (k-1, n-k-1)$

### Coeficiente de determinación

Al igual de lo que aprendimos en la RLS la bondad de ajuste del modelo de RLM se valora con el coeficiente de determinación ($R^2$), que nos dice qué proporción de la variabilidad de $Y$ es explicada por los coeficientes de la regresión del modelo en estudio. El $R^2$ es útil para comparar entre modelos

$$R^2 = \frac{SCT-SCE}{SCT}=\frac{SCR}{SCT}$$
Sin embargo, en el caso de la RLM, en donde deseamos incluir en el modelo más de una variable independiente, el $R^2$ siempre va a mejorar al agregar una nueva variable, aunque su inclusión no mejore sustancialmente el modelo. El $R^2$ ajustado o corregido por los grados de libertad valora la mejoría en explicar la variabilidad a pesar de haber incorporado más 

$$R^2_a = 1 - \bigg [ \frac{n-1}{n-(k+1)}\bigg]\frac{SCE}{SCT}=1-\bigg[\frac{n-1}{n-(k+1)}\bigg](1-R^2) $$

El $R^2$ más grande se obtiene por el simple hecho de incluir todas las variables disponibles, pero la mejor ecuación de regresión múltiple no necesariamente utiliza todas las variables. A causa de esta desventaja, la comparación de diferentes ecuaciones de regresión múltiple se logra mejor con el coeficiente ajustado de determinación, que es $R^2$ ajustado para el número de variables y el tamaño de la muestra.


::: {.b--gray .ba .bw2 .ma2 .pa4 .shadow-1}

Cuando tenemos que elegir el mejor modelo será necesario utilizar distintos criterios para compararlos y basar nuestra decisión en elegir el modelo que mejor explique la variación de $Y$ con el menor número de variables independientes, el modelo más simple y efectivo $\rightarrow$ también llamado el modelo más parsimonioso.

:::





## Ejemplo práctico en lenguaje R

En el documento incial de Estudios ecológicos vimos como generar en R una fórmula para la regresión lineal simple:

$$lm \;(variable\_dependiente \; \sim variable\_independiente)$$

Para generar fórmulas que contengan más de una variable independiente o predictora (necesario para que sea una regresión lineal múltiple) debemos agregarlas mediante el símbolo __+__

$$lm \;(variable\_dependiente \; \sim variable\_indepen1 \; + \; variable\_indepen2 \; + \; \dots \; + \; variable\_idepen\_n)$$

Si luego de la ~ incluímos **un punto** como notación de "todas", estamos creando un *modelo saturado* con todas las variables incluidas dentro del dataframe.

$$lm \;(variable\_dependiente \; \sim \;. \;, \; data)$$

Esto es útil cuando tenemos muchas posibles variables explicativas y queremos conocer cuáles tienen una correlación significativa.

También se puede descartar alguna o algunas variables explicativas en particular basado en la misma estructura, mediante el símbolo __-__

$$lm \;(variable\_dependiente \; \sim \;. \;  -variable\_indepen\_x, \; data)$$

En la línea anterior, incluimos dentro del modelo a todas las variables de **data** menos **variable_indepen_x**.

A continuación mostraremos un ejemplo en lenguaje R a partir del dataset ficticio utilizado anteriormente llamado **cardio**.

El conjunto de datos contiene datos agregados sobre el porcentaje de
personas que van en bicicleta al trabajo cada día, porcentaje de fumadores y porcentaje de personas con cardiopatías en una muestra imaginaria de 498 ciudades de un país determinado.

Los pasos que trataremos, luego de leer la tabla de datos, son:

1. Análisis de variables potencialmente explicativas
2. Estrategia de construcción del modelo
3. Comparación de modelos
4. Colinealidad
5. Diagnóstico del modelo (análisis de residuos)
6. Resumen del mejor modelo elegido

### Lectura de datos y visualización de estructura

Leemos los datos guardados en `cardiopatias.csv`:

```{r, message=F, warning=F}
library(tidyverse) # activamos paquete tidyverse

cardio <- read_csv("cardiopatias.csv")
```

Vemos su estructura:

```{r}
glimpse(cardio)
```
La tabla de datos contiene 4 variables (una de ellas ID_ciudad no analizable) y 498 observaciones.

### Análisis de variables potencialmente explicativas

En este pequeño ejemplo sólo tenemos dos variables potencialmente explicativas (ciclistas y fumadores). 

Analizaremos si hay relación entre ellas, y entre cada una y la variable dependiente mediante una matriz de correlación.

Existen muchas funciones para llevar a cabo esta tarea en R. Dado que mostramos previamente la función `cor()` y `cor.test()` de R base, en esta oportunidad presentaremos otras funciones más elaboradas pertenecientes al paquete **dlookr**.

```{r, message=F, warning=F, fig.align='center', out.width = "60%"}
library(dlookr)

correlate(cardio[-1]) %>% 
  arrange(desc(abs(coef_corr)))

plot_correlate(cardio, -ID_ciudad)
```
En los argumentos de las funciones omitimos la variable ID_ciudad y dejamos todas las otras. Encontramos de forma analítica y gráfica que existe una alta correlación inversa entre cardiopatías y ciclistas (-0,93), una baja relación directa entre cardiopatías y fumadores (0,31) y una correlación inexistente entre ciclistas y fumadores (0,01).

Otra función interesante es `ggcorrmat()` del paquete **ggstatsplot** (que debemos tener instalado previamente).


```{r, message=F, warning=F, fig.align='center', out.width = "60%"}
library(ggstatsplot)

ggcorrmat(cardio)
```

Además del coeficiente de correlación de Pearson y los colores con sus intensidades agrega de modo directo la significación (valores tachados por una X no son significativos).

### Estrategia de construcción del modelo

A la hora de seleccionar las variables independientes que deben formar parte del modelo se pueden seguir varios métodos:

__Método jerárquico (forward)__: se basa en el criterio del investigador que introduce predictores determinados en un orden específico en relación al marco teórico. Es una estrategia forward manual, dado que se comienza con un modelo mínimo o nulo y se van incorporando variables.

__Método de entrada forzada (backward)__: es el método inverso al anterior. Se introducen todos los predictores simultáneamente y luego se van quitando los que no son significativos. Es una estrategia backward manual.

__Método paso a paso (stepwise)__: emplea criterios matemáticos para decidir qué predictores contribuyen significativamente al modelo y en qué orden se introducen.

Un índice común utilizado en lenguaje R es Akaike Information Criterion (AIC) que ajusta mediante *máxima verosimilitud* (función `AIC()`). El algoritmo AIC busca el modelo equilibrado que describa adecuadamente la relación y tenga el mínimo AIC (es decir, cuanto más pequeño es el índice mejor es el modelo).

Dentro de este método automático se diferencian tres estrategias:

__Dirección forward__: El modelo inicial no contiene ningún predictor, solo el intercepto ($\beta_0$). A partir de este se generan todos los posibles modelos introduciendo una sola variable de entre las disponibles. Aquella variable que mejore en mayor medida el modelo se selecciona. A continuación se intenta incrementar el modelo probando a introducir una a una las variables restantes. Si introduciendo alguna de ellas mejora, también se selecciona. En el caso de que varias lo hagan, se selecciona la que incremente en mayor medida la capacidad del modelo. Este proceso se repite hasta llegar al punto en el que ninguna de las variables que quedan por incorporar mejore el modelo.

__Dirección backward__: El modelo se inicia "saturado", es decir con todas las variables disponibles incluidas como predictores. Se prueba a eliminar una a una cada variable, si se mejora el modelo, queda excluida. Este método permite evaluar cada variable en presencia de las otras.

__Doble o mixto__: Se trata de una combinación de la selección forward y backward. Se inicia igual que el forward pero tras cada nueva incorporación se realiza un test de extracción de predictores no útiles como en el backward. Presenta la ventaja de que si a medida que se añaden predictores, alguno de los ya presentes deja de contribuir al modelo, se elimina.

La función `step()` en R base permite encontrar el mejor modelo basado en AIC utilizando cualquiera de las 3 variantes automáticas del método paso a paso. 

Siempre es oportuno aclarar que estos últimos métodos se basan en cálculos matemático/estadísticos que no tienen en cuenta criterios conceptuales epidemiológicos que surjan del marco teórico, por lo que exigen un control especial del analista.

Como siempre el lenguaje R ofrece una variedad de funciones (algunas base y otras provenientes de paquetes adicionales) para abordar y facilitar la tarea de seleccionar el mejor modelo de regresión (aquel que mejor explique la relación entre variables y que a la vez sea el más simple - principio de parsimonia).

El índice de bondad de ajuste utilizado para compararlos, además del AIC, es el   $R^2 \; ajustado$.

Como consecuencia de tener solo dos variables independientes posibles y a modo didáctico, vamos a crear tres objetos de regresión, dos de ellos regresiones lineales simples y uno de regresión lineal múltiple:

```{r}
mod_simple1 <- lm(cardiopatias ~ ciclistas, data = cardio)

mod_simple2 <- lm(cardiopatias ~ fumadores, data = cardio)

mod_multiple <- lm(cardiopatias ~ ciclistas + fumadores, data = cardio)
```

Mediante `summary()` observamos sus resultados:

```{r}
summary(mod_simple1)
summary(mod_simple2)
summary(mod_multiple)
```
En los resúmenes observamos que todos los coeficientes de las variables involucradas son significativas y que los valores mostrados tienen forma de lista, es decir no son tablas "ordenadas" por lo que muchas veces, sobretodo cuando trabajamos con numerosas variables, se hace difícil la comparación de resultados.

### Comparación de modelos

Algunas funciones vienen a aportar una solución sencilla para la comparación de modelos. Es el caso de `compare_models()` del paquete **paremeters** y `compare_performance()` del paquete **performance**.

```{r, message=F, warning=F}
library(parameters)

compare_models(mod_simple1, mod_simple2, mod_multiple)
```
Se muestran para cada modelo, los coeficientes y sus IC.


```{r, message=F, warning=F}
library(performance)

compare_performance(mod_simple1, mod_simple2, mod_multiple, 
                    metrics = "common")
```
Las métricas comunes extraídas de los objetos de regresión, presentadas en una tabla, son:

- **AIC**: Criterio de información de Akaike

- **BIC**: Criterio de información bayesiano

- **R2**: Coeficiente de determinación $R^2$

- **R2 (adj.)**: Coeficiente de determinación ajustado

- **RMSE**: Error cuadrático medio

En este ejemplo, vemos que el modelo simple de ciclistas es mucho mejor que el modelo simple de fumadores, pero el modelo múltiple que incluye ambas variables tiene aún mejores índices (menor AIC y RMSE y mayor $R^2$ ajustado. 


### Colinealidad

Hay varias formas de intuir si hay colinealidad, es decir relación lineal entre nuestras variables independientes. 

La primera es analizar el coeficiente de correlación; si tenemos variables altamente relacionadas es muy probable que el modelo pueda tener colinealidad entre esas variables independientes. 

Otro de los síntomas se produce cuando nuestro modelo tiene un alto coeficiente de correlación ($R^2$) y muchas variables no son significativas. En estos casos también es muy probable la existencia de colinealidad.

Cuando hemos intuido que tenemos multicolinealidad y queremos comprobar, el paquete __car__ nos ofrece una función que implementa el método VIF (variance inflation factor). 

Este factor cuantifica la intensidad de la multicolinealidad en un análisis de regresión normal de mínimos cuadrados y se utiliza así:


```{r, message=F, warning=F}
library(car)

vif(mod_multiple)
```

Los resultados del ejemplo muestra VIF bajos, cerca del 1 que es el valor más bajo del índice (no hay colinealidad).

El umbral de detección parte de valores 4 o 5, donde debería llamarnos la atención. Un valor 10 o superior indicaría que el modelo de regresión lineal presenta un grado de multicolinealidad preocupante.

### Diagnóstico del modelo (análisis de residuos)

El diagnóstico final del modelo elegido lo debemos realizar con sus residuos.

La forma gráfica habitual es "plotear" el objeto de regresión.

```{r, fig.align='center', out.width = "60%"}
plot(mod_multiple)
```
Por otra parte, también tenemos las funciones de análisis para los supuestos (independencia, linealidad, normalidad, homocedasticidad)

__Independencia con estadístico de Durbin-Watson__

```{r, message=F, warning=F}
library(lmtest)

dwtest(mod_multiple)
```
El valor de p es de 0,1773 por lo que no podemos rechazar la hipótesis nula de inexistencia de autocorrelación (independencia)


**Linealidad con estadístico Ramsey's RESET**

```{r, message=F}
resettest(mod_multiple)
```
El p valor es de 0,2372 por lo que no podemos rechazar la hipótesis nula de linealidad.

__Normalidad de los residuos con test de Lilliefors__

```{r, message=F, warning=F}
library(nortest)

lillie.test(mod_multiple$residuals)
```
Los resultados del test nos confirman lo visto en el grafico Q-Q,  el valor p es de 0,2681 y no podemos descartar normalidad de los residuos.

__Homocedasticidad con test de Breush-Pagan__


```{r}
bptest(mod_multiple)
```
Cumple con el supuesto de homocedasticidad (valor p al límite mayor a 0,05)

Finalmente presentamos un paquete interesante para validar supuestos de modelos lineales denominado **gvlma**.

Su función, de mismo nombre `gvlma()`, implementa un procedimiento global sobre vector residual estandarizado para probar los cuatro supuestos del modelo lineal. 

Si el procedimiento global indica una violación de al menos uno de los supuestos, los componentes se pueden utilizar para obtener información sobre qué supuestos se han violado.

```{r, message=F, warning=F}
library(gvlma)

gvlma(mod_multiple)
```

Todos los resultados son aceptables.

### Resumen del mejor modelo elegido

```{r}
summary(mod_multiple)
```

El modelo **mod_multiple** es capaz de explicar el 97,95 % de la variabilidad observada en la proporción de cardiopatías de estas ciudades. $R^2$-Adjustado: 0,9795). El test F global muestra que es significativo (p-value: < 2.2e-16). 

El coeficiente de pendiente de la recta para la proporción de ciclistas fue de -0,20 por lo que la proporción de cardiopatías disminuye un 0,2 % por cada 1 % que sube la proporción de ciclistas manteniendo constante la variable fumadores.
Por otra parte, fumadores tiene un coeficiente de 0,17, por lo que la proporción de cardiopatías aumenta un 0,17 % por cada 1 % que sube la proporción de fumadores de la ciudad manteniendo constante la variable ciclistas.

Se satisfacen todas las condiciones para la regresión lineal. 


### Funciones para modelado automático

El uso de la función `step()` se enmarca en los procedimientos automáticos y puede ser forward, backward o mixto.

La sintaxis básica de la función es:

> step(objeto, direction)

donde:

__objeto__ es el modelo inicial de regresión lineal (nulo o saturado)

__direction__ es la dirección que indicamos. Puede ser "forward", "backward" o "both" (predeterminado, si no se define)  

Generalmente conviene partir del modelo saturado y utilizar la dirección por defecto, donde se usan ambas direcciones.


```{r}
modelo_saturado <- lm(cardiopatias ~ .-ID_ciudad, data = cardio)

modelo_step <- step(modelo_saturado, direction = "both") # el argumento direction se puede omitir
```
Observamos que arribamos al mismo resultado con las dos variables, esperable dado la cantidad mínima de variables para la regresión múltiple.

```{r}
summary(modelo_step)
```


### Inclusión de variables categóricas

Si bien en este ejemplo no tenemos variables categóricas, cuando debemos introducirlas como independientes, una categoría (habitualmente denominado nivel) de la variable se considera el nivel de referencia (normalmente codificado como 0) y el resto de los niveles se comparan con él. 

En el caso que el predictor categórico tenga más de dos niveles, se generan lo que se conoce como variables `dummy`, que son variables creadas para cada uno de los niveles del predictor categórico y que pueden tomar el valor de 0 o 1. 

Cada vez que se emplee el modelo para predecir un valor, solamente una variable *dummy* por predictor adquiere el valor 1 (la que coincida con el valor que adquiere el predictor en ese caso) mientras que el resto se consideran 0. 

El valor del coeficiente parcial de regresión $\beta_i$ de cada variable *dummy* indica la proporción promedio en el que influye dicho nivel sobre la variable dependiente $Y$ en comparación con el nivel de referencia de dicho predictor.

Las consideraciones técnicas dentro de R para que el lenguaje se encargue de construir las variables dummy automáticamente de forma interna es trabajar con **factores**.

Los tipos de datos **factor** tienen una estructura de niveles donde el primero de ellos es el de referencia. Para modificar el nivel de referencia podemos utilizar la función `relevel()` de R base. 


## Bibliografía

Daniel W. Bioestadística: Base para el análisis de las ciencias de la salud. 4° Edición. Ed. Limusa Wiley, 2002.

Ballester F. y Tenías Burrillo J. M. Estudios ecológicos. Quaderns de Salut Pública i Administració de Serveis de Salut 20. Valencia: Escola Valenciana d´Estudis per a la Salut, 2003.

Epidemiología. Diseño y análisis de estudios. Mauricio Hernández Ávila. Editorial Médica Panamericana, 2007.

Escuela Nacional de Sanidad (ENS). Instituto de Salud Carlos III. Ministerio de Ciencia e Innovación. Manual Docente de la Escuela Nacional de Sanidad .Método epidemiológico. Madrid, 2009.

Triola, M. Estadística. 10° Edición. Pearson Educación. México. 2009.

Simon J. Sheather, A Modern Approach to Regression with R. Department of Statistics, College Station, TX, USA, Springer Science+Business Media, LLC, 2009

Sanford Weisberg, Applied Linear Regression. School of Statistics, University of Minnesota. John Wiley & Sons, Inc, 2005

R Core Team. R: A language and environment for statistical
computing. R Foundation for Statistical Computing, Vienna, Austria, 2021. URL
<https://www.R-project.org/>.

Choonghyun Ryu. dlookr: Tools for Data Diagnosis, Exploration, Transformation. R package version 0.4.4. 2021. https://CRAN.R-project.org/package=dlookr

Juergen Gross and Uwe Ligges. nortest: Tests for Normality. R package
version 1.0-4. 2015 https://CRAN.R-project.org/package=nortest

Wickham et al. Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, Año 2019 https://doi.org/10.21105/joss.01686

Achim Zeileis, Torsten Hothorn. Diagnostic Checking in Regression Relationships. R News 2(3), 7-10. Año 2002. URL https://CRAN.R-project.org/doc/Rnews/

Patil, I. ggstatsplot: 'ggplot2' Based Plots with Statistical Details., 2018 CRAN. Retrieved from https://cran.r-project.org/web/packages/ggstatsplot/index.html

John Fox and Sanford Weisberg. An {R} Companion to Applied Regression, Third Edition. Thousand Oaks CA: Sage., 2019 URL:   https://socialsciences.mcmaster.ca/jfox/Books/Companion/

Edsel A. Pena and Elizabeth H. Slate. gvlma: Global Validation of
Linear Models Assumptions. R package version 1.0.0.3., 2019 https://CRAN.R-project.org/package=gvlma

Lüdecke D, Ben-Shachar M, Patil I, Makowski D. “Extracting, Computing
and Exploring the Parameters of Statistical Models using R.” _Journal of
Open Source Software_, *5*(53), 2445., 2020 doi: 10.21105/joss.02445 (URL:
https://doi.org/10.21105/joss.02445).